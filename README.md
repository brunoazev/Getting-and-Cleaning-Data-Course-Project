# **Getting and Cleaning Data - Course Project**
*This course is part of the Data Science Specialization offered by Johns Hopkins through Coursera.*

The purpose of this project is to demonstrate my ability to collect, work with, and clean a data set.  The goal was to prepare tidy data that can be used for later analysis. 
### Introduction
---

Coursera intro about wearable computing:
> One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones

A quick overview about the experiment in which the raw data has been collected:

>The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

### Repo included files
---
- **UCI HAR Dataset**: folder containing the raw data about the experiment described above;
- **run_analysis.R**: main script developed in R to perform analysis on the raw data and generate a tidy data text file;
- **getTidyFeatures.R**: script sourced in run_analysis.R, containing a function for transforming the data (features.txt) describing the features;
- **getTidyData.R**: script sourced in run_analysis.R, containing a function for transforming the data corresponding to the values measured in the experiment;
- **codebook.md**: the code book, a dictionary which indicates all the variables and relevant information to understand the data;
- **README.md**: this file you are reading, which explains the analysis files and all processes involved;
- **tidydata.txt**: the tidy data generated by the run_analysis.R script.

#### UCI HAR Dataset (Folder)
---
For more information about the raw data, please consider reading the README.txt included in this folder.

#### run_analysis.R (File)
---
This is the main script for generating the tidy data. 

It is intended to perform the following steps on the raw data:

1. Merges the training and the test sets to create one data set.
2. Extract only the measurements on the mean and standard deviation for each measurement.
3. Use descriptive activity names to name the activities in the data set
4. Appropriately label the data set with descriptive variable names.
5. From the data set in step 4, create a second, independent tidy data set with the average of each variable for each activity and each subject.

To sum up, the goal of this script is to merge the X_train and X_test data generating a single tidy dataset. 

##### Fixing the features data from features.txt file
The first part of the script fixes the features data, loaded from the file *features.txt*.
The data was loaded with the following command, where the variable "featureFile" is the path of features.txt:
```R
features <- read.table(featureFile, sep=" ", col.names = c("featureid","feature"))
```
 To make the features data become more descriptive and easier to understand, it was created a function in the **getTidyFeatures.R** file which is sourced in the beggining of the code. This function gets the second column of the data in *features.txt* which contains the name of the feature and divides it into 4 new columns, according to the information that could be extracted considering the information of the features_info.txt file included in the UCI HAR Dataset folder.
*More information of how the data was exctracted are described in the code section. All code was comented in order to make it clear and understandable for the reader*.


Let's see the first four rows of the data before and after the transformation:

Before
```R
head(features,4)
```
<table>
<thead>
<tr><td></td><td>featureid</td><td>feature</td></tr>
</thead>
<tr><td>1</td><td>1</td><td>tBodyAcc-mean()-X</td></tr>
<tr><td>2</td><td>2</td><td>tBodyAcc-mean()-Y</td></tr>
<tr><td>2</td><td>3</td><td> tBodyAcc-mean()-Z</td></tr>
<tr><td>2</td><td>4</td><td>tBodyAcc-std()-X</td></tr>
</table>

After

<table>
<thead>
<tr><td></td><td>featureid</td><td>feature</td><td>measurement</td><td>description</td><td>calculation</td><td>direction</td></tr>
</thead>
<tr><td>1</td><td>1</td><td>tBodyAcc-mean()-X</td><td>time</td><td>Body Accelerometer - X - mean - time</td><td>mean</td><td>X</td></tr>
<tr><td>2</td><td>2</td><td>tBodyAcc-mean()-Y</td><td>time</td><td>Body Accelerometer - Y - mean - time</td><td>mean</td><td>Y</td></tr>
<tr><td>2</td><td>3</td><td> tBodyAcc-mean()-Z</td><td>time</td><td>Body Accelerometer - Z - mean - time</td><td>mean</td><td>Z</td></tr>
<tr><td>2</td><td>4</td><td>tBodyAcc-std()-X</td><td>time</td><td>Body Accelerometer - X - standard deviation - time</td><td>standard deviation</td><td>X</td></tr>
</table>

##### Putting all together: train and test data
The X_train.txt and X_test.txt files contain data of same structure with only numeric values corresponding to the observations for the different features measured on the experiment (e.g: the body gyroscope mean time calculated on X axis). Each column of these data represents the feature measured and each row corresponds to an observation for some volunteer activity performed (e.g: volunteer number 1 sitting at some period of time).
The following table shows part (first two rows and first two columns) of the X_train.txt file after loaded into R (assume we named the data frame as "xData"):
```R
head(xData[1:2,1:2])
```
<table>
<thead>
<tr><td></td><td>V1</td><td>V2</td></tr>
</thead>
<tr><td>1</td><td>0.2885845</td><td>-0.02029417</td></tr>
<tr><td>2</td><td>0.2784188</td><td>-0.01641057</td></tr>
</table>


The y_train.txt and y_test.txt contain labels for X_train and X_test that represent the activity performed on each observation (WALKING, SITTING, etc.). Finally, the subject_train.txt and subject_test.txt files contain representations for the volunteer who participated in the experiment. The **getTidyData.R** file has the function to get all these information gathered together producing a more readable data for both X_train and X_test. Besides that, as needed a new column was added to keep the number of record of each activity performed. All the process for transforming the X_train and X_test 
At this point, the data is presented as following, for the first two rows and first five columns for volunteer number one:

```R
head(xData[1:2,1:5])
```
<table>
<thead>
<tr><td></td><td>volunteernumber</td><td>recordnumber</td><td>activity</td><td>V1</td><td>V2</td></tr>
</thead>
<tr><td>1</td><td>1</td><td>1</td><td>STANDING</td><td>0.2885845</td><td>-0.02029417</td></tr>
<tr><td>2</td><td>1</td><td>2</td><td>STANDING</td><td>0.2784188</td><td>-0.01641057</td></tr>
</table>

The same data for volunteer number three:

<table>
<thead>
<tr><td></td><td>volunteernumber</td><td>recordnumber</td><td>activity</td><td>V1</td><td>V2</td></tr>
</thead>
<tr><td>1</td><td>1</td><td>1</td><td>STANDING</td><td>0.2885845</td><td>-0.02029417</td></tr>
<tr><td>2</td><td>1</td><td>2</td><td>STANDING</td><td>0.2784188</td><td>-0.01641057</td></tr>
</table>

Another process in tyiding the data was transposing the features (V1:V561) columns to rows, using the gather function from tidyr library:
```R
xData <- xData %>% gather(featureid,value,V1:V561)
```
Look at how the data is now presented:

<table>
<thead>
<tr><td></td><td>volunteernumber</td><td>recordnumber</td><td>activity</td><td>featureid</td><td>value</td></tr>
</thead>
<tr><td>1</td><td>1</td><td>1</td><td>STANDING</td><td>V1</td><td>0.2885845</td></tr>
<tr><td>2</td><td>1</td><td>1</td><td>STANDING</td><td>V2</td><td>-0.02029417</td></tr>
</table>
*The data above was filtered to show the most relevant information to comprehension of the transpose process*.

Finally, the last process was replacing the column featureid with the respective feature description from the features data frame. For that, it was necessary to remove the 'V' character from it, cast it to integer, and then match the featureid of both data frames (xData and features) using *left_join()* function. This process will join the two data frames, so the last code line in getTidyData function selects only the relevant columns:


 After doing all the necessary process to make the data tidy, the last thing to do is calculate the average of each variable for each activity and each subject, and generate the file for submission.

### The code
---
Initialization:
```R
  #Loads useful libraries 
  library(plyr)
  library(dplyr)  
  library(stringr)
  library(tidyr)
  
  #Loads external scripts
  source("getTidyData.R")
  source("getTidyFeatures.R")
  
  #Saves the current directory
  oldwd <- getwd()
  #Goes to the directory with the original data
  setwd("UCI HAR Dataset")

  #Sets useful variables 
  featureFile <- "features.txt"
  activityFile <- "activity_labels.txt"
  xTrainFile <- "X_train.txt"
  yTrainFile <- "y_train.txt" 
  xTestFile <- "X_test.txt"
  yTestFile <- "y_test.txt"
  subjectTrainFile <- "subject_train.txt"
  subjectTestFile <- "subject_test.txt"
  dirTrain <- "train"
  dirTest <- "test"
  dirBack <- ".."
  xTrainData <- NULL
  xTestData <- NULL
  tidyData <- NULL  
  features <- c()
```

Processing, merging, and getting the tidy data:
``` R
    tryCatch({
        if(file.exists(featureFile)){
          #Loads the 'features' data and makes it tidy 
          features <- getTidyFeatures(featureFile)
        }
        
        if(file.exists(activityFile)){
          #Loads the information from the file activity_labels.txt to the 'activities' variable
          activities <- read.table(activityFile, sep=" ", col.names = c("id","description"))
        }
        
        if(dir.exists(dirTrain)){
          #Go to the "train" directory
          setwd(dirTrain)
          if(file.exists(yTrainFile) && file.exists(xTrainFile) && file.exists(subjectTrainFile)){
            #Loads the 'X_train.txt' file into a data frame and makes it tidy 
            xTrainData <- getTidyData(xTrainFile,yTrainFile,subjectTrainFile, activities, features)
            #Adds a column to make it possible knowing the data type (test/train)
            xTrainData <- cbind(datatype="train",xTrainData)
          }
          #Sets current directory to previous directory
          setwd(dirBack)
        }
        
        if(dir.exists(dirTest)){
          #Go to the "test" directory
          setwd(dirTest)
          if(file.exists(yTestFile) && file.exists(xTestFile) && file.exists(subjectTestFile)){
            #Loads the 'X_test.txt' file into a data frame and makes it tidy 
            xTestData <- getTidyData(xTestFile,yTestFile,subjectTestFile, activities, features)
            #Adds a column to make it possible knowing the data type (test/train)
            xTestData <- cbind(datatype="test",xTestData)
          }
          #Sets current directory to previous directory
          setwd(dirBack)
        }
        
        #Merges training and test sets (requirement 1)
        tidyData<-rbind(xTrainData,xTestData)
        #Creates a new tidy data set with the average of each feature for each activity and each subject
        tidyDataSummarised <- ddply(tidyData,.(volunteernumber,activity,feature),summarise,mean=mean(value))
        
        #Sets the current directory to root directory
        setwd(dirBack)
        
        #Writes the data for submission
        write.table(tidyDataSummarised,"tidydata.txt",row.names = FALSE)
        
      }, finally={
        #In any case sets the current directory back to its initial value
        setwd(oldwd)
      })
```

### Running the analysis
---
To run this analysis and generate the tidy data on your own, you just need to follow these steps:

1. Clone the repository in your machine
2. Start RStudio or R
3. Set the folder where the repository was cloned in step 1 as current directory (see *?setwd* for help)
4. Load the run_analysis function in your enviroment. To do it just type 
	
	```R
	source("run_analysis.R")
	```
5. Call the run_analysis function typing
	
	```R
	run_analysis()
	```

### Reading the tidy data generated
---

Use the following function to read the tidy data generated into R:

data <- read.table("tidydata.txt",header=TRUE)
