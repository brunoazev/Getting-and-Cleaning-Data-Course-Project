# Getting and Cleaning Data - Course Project
*This course is part of the Data Science Specialization offered by Johns Hopkins through Coursera.*

The purpose of this project is to demonstrate my ability to collect, work with, and clean a data set.  The goal was to prepare tidy data that can be used for later analysis. 
### Introduction
---

Coursera intro about wearable computing:
> One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones

A quick overview about the experiment in which the raw data has been collected:

>The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

### Repo included files
---
- **UCI HAR Dataset**: folder containing the raw data about the experiment described above;
- **run_analysis.R**: main script developed in R to perform analysis on the raw data and generate a tidy data text file;
- **getTidyFeatures.R**: script sourced in run_analysis.R, containing a function for transforming the data (features.txt) describing the features;
- **getTidyData.R**: script sourced in run_analysis.R, containing a function for transforming the data corresponding to the values measured in the experiment;
- **codebook.md**: the code book, a dictionary which indicates all the variables and relevant information to understand the data;
- **README.md**: this file you are reading, which explains the analysis files and all processes involved;
- **tidydata.txt**: the tidy data generated by the run_analysis.R script.

#### UCI HAR Dataset
For more information about the raw data, please consider reading the README.txt included in this folder.

### run_analysis.R 

This is the main script for generating the tidy data. 
It is intended to perform the following actions:
1. Merges the training and the test sets to create one data set.
2. Extract only the measurements on the mean and standard deviation for each measurement.
3. Use descriptive activity names to name the activities in the data set
4. Appropriately label the data set with descriptive variable names.
5. From the data set in step 4, create a second, independent tidy data set with the average of each variable for each activity and each subject.

To sum up, the goal of this script is to merge the X_train and X_test data generating a single tidy dataset. 
The first part of the script is to fix the feature data and make it become more descriptive and easy to understand.
The X_train and X_test data are only numeric values of the observations for different features measured. The y_train and y_test data are labels that represent the activity performed on each observation (WALKING, SITTING, etc.), so all these information are gathered and included in the tidy data.
The subject_train and subject_test data are representations of the volunteer who participated in the experiment, so this information is also included in the final data. After doing all the necessary process to make the data tidy, the last thing to do is calculate the average of each variable for each activity and each subject, and generate the file for submission.

The 

##### The code
Initialization:
```R
  #Loads useful libraries 
  library(plyr)
  library(dplyr)  
  library(stringr)
  library(tidyr)
  
  #Loads external scripts
  source("getTidyData.R")
  source("getTidyFeatures.R")
  
  #Saves the current directory
  oldwd <- getwd()
  #Goes to the directory with the original data
  setwd("UCI HAR Dataset")

  #Sets useful variables 
  featureFile <- "features.txt"
  activityFile <- "activity_labels.txt"
  xTrainFile <- "X_train.txt"
  yTrainFile <- "y_train.txt" 
  xTestFile <- "X_test.txt"
  yTestFile <- "y_test.txt"
  subjectTrainFile <- "subject_train.txt"
  subjectTestFile <- "subject_test.txt"
  dirTrain <- "train"
  dirTest <- "test"
  dirBack <- ".."
  xTrainData <- NULL
  xTestData <- NULL
  tidyData <- NULL  
  features <- c()
``` 
Processing, mergin, and getting the tidy data:

    tryCatch({
        if(file.exists(featureFile)){
          #Loads the 'features' data and makes it tidy 
          features <- getTidyFeatures(featureFile)
        }
        
        if(file.exists(activityFile)){
          #Loads the information from the file activity_labels.txt to the 'activities' variable
          activities <- read.table(activityFile, sep=" ", col.names = c("id","description"))
        }
        
        if(dir.exists(dirTrain)){
          #Go to the "train" directory
          setwd(dirTrain)
          if(file.exists(yTrainFile) && file.exists(xTrainFile) && file.exists(subjectTrainFile)){
            #Loads the 'X_train.txt' file into a data frame and makes it tidy 
            xTrainData <- getTidyData(xTrainFile,yTrainFile,subjectTrainFile, activities, features)
            #Adds a column to make it possible knowing the data type (test/train)
            xTrainData <- cbind(datatype="train",xTrainData)
          }
          #Sets current directory to previous directory
          setwd(dirBack)
        }
        
        if(dir.exists(dirTest)){
          #Go to the "test" directory
          setwd(dirTest)
          if(file.exists(yTestFile) && file.exists(xTestFile) && file.exists(subjectTestFile)){
            #Loads the 'X_test.txt' file into a data frame and makes it tidy 
            xTestData <- getTidyData(xTestFile,yTestFile,subjectTestFile, activities, features)
            #Adds a column to make it possible knowing the data type (test/train)
            xTestData <- cbind(datatype="test",xTestData)
          }
          #Sets current directory to previous directory
          setwd(dirBack)
        }
        
        #Merges training and test sets (requirement 1)
        tidyData<-rbind(xTrainData,xTestData)
        #Creates a new tidy data set with the average of each feature for each activity and each subject
        tidyDataSummarised <- ddply(tidyData,.(volunteernumber,activity,feature),summarise,mean=mean(value))
        
        #Sets the current directory to root directory
        setwd(dirBack)
        
        #Writes the data for submission
        write.table(tidyDataSummarised,"tidydata.txt",row.names = FALSE)
        
      }, finally={
        #In any case sets the current directory back to its initial value
        setwd(oldwd)
      })
'''

### Running the analysis
---
To run this analysis and generate the tidy data on your own, you just need to follow these steps:
1. Clone the repository in your machine
2. Start RStudio or R
3. Set the folder where the repository was cloned in step 1 as current directory (see *?setwd* for help)
4. Load the run_analysis function in your enviroment. To do it just type *source("run_analysis.R")*.
5. Call the run_analysis function typing *run_analysis()*

### Reading the tidy data generated
---

Use the following function to read the tidy data generated into R:

data <- read.table("tidydata.txt",header=TRUE)



